{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Okt as Twitter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# G마켓에 좋은 평만 있어서 사봤는데.. 정말 진짜 진짜 사지마세요. 개. 쓰. 레. 기 (진심) 입니다. 액정부터 짜증나는 TN패널에, 하드 SSD인걸로 알았는데, 속도는 저질 SD카드 꽂아 놓은것 같습니다. 정말 느려터집니다. 저는 단지 인터넷 뱅킹만 할려고, 샀단 말입니다. 그런데 인터넷 뱅킹 프로그램까는데만 10~20분 걸립니다. 뭐약!! 이게!! 분노로 인해 볼때마다 짜증납니다. 밤에 잠도 안오고요.. 사시면 분명 후회하실겁니다. 아! 진짜 G마켓 프리미엄평으로 실날하게 사진찍어서 올리려고했는데, 먹고 산다고 바빠서 프리미엄 평 못 올린게 정말 천추의 한이네요!!\\n# 원래 그런 줄 알고 사는 \"저가 제품\"이라고 생각합니다만. IPS라는 언급이 없으니 당연히 TN 패널일 테고, EMMC는 SSD가 아니고 SD 카드 내장된 것 같은 것이라 원래 SSD보다 느린 것이고, CPU도 아톰이니 뭐 당연히 느리죠. 그런 것 다 감안하고 \"싸고 가볍다\"는 조건으로 사는 제품인데요. 뭐 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = codecs.open('data/reviews.txt', 'r', 'utf-8')\n",
    "f.read()[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"발열\", \"소음\"]\n",
    "\n",
    "for keyword in keywords:\n",
    "    temp_list = []\n",
    "    save_name = \"data/reviews_\" + keyword + \".txt\"\n",
    "    f = codecs.open(\"data/reviews.txt\", \"r\", \"utf-8\")\n",
    "    t = codecs.open(save_name, \"w\", \"utf-8\")\n",
    "    \n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        if keyword in line:\n",
    "            temp_list.append(line)\n",
    "    set_list = list(set(temp_list))\n",
    "    \n",
    "    for item in set_list:\n",
    "        t.write(item)\n",
    "\n",
    "    f.close()\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 살려고 고민했는데 잘산거 같아요 쿨러 소리만 조금더 조용하면 진짜 좋을텐데 다른제품 쿨러 비교는 못해봐서 모르겠는데 쿨러소음이 없는걸 구입하고 싶어요\\n# 안녕하십니까. 레노버 유통사 반석전자 입니다. 팬소음 이슈는 크게 없는 모델입니다. 이용하시고자 하는 프로그램의 권장사양을 확인해주시기 바라며 동일사양의 플레이 영상을 참조해주시기 바랍니다. https://goo.gl/UKTTh1 ( * GTX 950m + i5-6300HQ ) 감사합니다.\\n# 큰맘먹고 노트북 처음샀는데 받자마자 보는순간 너무너무 이쁘네요 발열도 없고 소음도 진짜 없고 너무가벼워서 진짜 잘산거같아요 ~~~\\n# 노트북은 처음인데 속도빠르고 소음작고 맘에듭니다 ^^\\n# 일단 기계는 양호한편입니다. 근데 소음이 심해요\\n# 일단좋네요 이쁘고 가격대비 사양도 좋구요. 팬소음은.. 음 쫌 거슬리네요. 이부분은 감안하셔야 할 것 같습니다.\\n# 좋습니다 게임은 디아블로3만해봤는데 끈킴없이 잘돌아갑니다 win10을 처음 사용해보'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = codecs.open(\"data/reviews_소음.txt\", 'r', 'utf-8')\n",
    "f.read()[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Scored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/score_소음.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "data = pd.read_excel(filename, sheet_name=sheet_name, header=0)\n",
    "\n",
    "csv_data = [item.replace('#', '').strip() for item in data['Review']]\n",
    "csv_label = data[\"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['소음.... 상당히 거슬립니다', '소음은 상당합니다', '소음은 좀 많네요', '소음이 심해요', '소음이 큰편입니다']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    2\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[698, 265, 132]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [sum(csv_label == 0), sum(csv_label == 1), sum(csv_label == 2)]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(scores)), scores, color = \"#ff5733\")\n",
    "plt.title(\"Average Laptop's Score Of Noise\")\n",
    "plt.xlabel(\"Noise Score\")\n",
    "plt.ylabel(\"Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "doc = []\n",
    "\n",
    "for sentence in csv_data:\n",
    "    results = []\n",
    "    tokens = twitter.pos(sentence, norm=True, stem=True)\n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "    doc.append(\" \".join(results).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['소음 상당하다 거슬리다', '소음 상당하다', '소음 좀 많다', '소음 심해', '소음 크다 편입 니']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv = CountVectorizer(ngram_range=(1, 1), min_df=3)\n",
    "\n",
    "data = cnv.fit_transform(doc).toarray()\n",
    "label = csv_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'소음': 127,\n",
       " '상당하다': 117,\n",
       " '거슬리다': 13,\n",
       " '많다': 74,\n",
       " '심해': 136,\n",
       " '크다': 212,\n",
       " '편입': 215,\n",
       " '빼다': 109,\n",
       " '심하다': 135,\n",
       " '나다': 34,\n",
       " '도서관': 52,\n",
       " '같다': 11,\n",
       " '쓸다': 143,\n",
       " '들다': 61,\n",
       " '않다': 151,\n",
       " '너무': 37,\n",
       " '업무': 160,\n",
       " '되다': 57,\n",
       " '정도': 188,\n",
       " '이다': 171,\n",
       " '보이다': 99,\n",
       " '발열': 91,\n",
       " '편이': 214,\n",
       " '약간': 153,\n",
       " '하지만': 220,\n",
       " '게이': 17,\n",
       " '노트북': 38,\n",
       " '하다': 218,\n",
       " '펴다': 213,\n",
       " '커서': 206,\n",
       " '놀라다': 39,\n",
       " '인터넷': 175,\n",
       " '검색': 16,\n",
       " '해보다': 221,\n",
       " '생각': 119,\n",
       " '단점': 47,\n",
       " '바로': 88,\n",
       " '조용하다': 193,\n",
       " '괜찮다': 22,\n",
       " '사다': 110,\n",
       " '없다': 161,\n",
       " '알다': 152,\n",
       " '충전': 205,\n",
       " '심해지다': 137,\n",
       " '돌아가다': 56,\n",
       " '소리': 126,\n",
       " '크게': 210,\n",
       " '심다': 134,\n",
       " '센터': 125,\n",
       " '이렇게': 172,\n",
       " '자다': 178,\n",
       " '부분': 101,\n",
       " '바라다': 87,\n",
       " '있다': 177,\n",
       " '정말': 189,\n",
       " '사용': 112,\n",
       " '현재': 223,\n",
       " '윈도우': 169,\n",
       " '가격': 3,\n",
       " '대비': 49,\n",
       " '사양': 111,\n",
       " '아니다': 144,\n",
       " '아주': 147,\n",
       " '들리다': 62,\n",
       " '그렇다': 27,\n",
       " '게임': 18,\n",
       " '돌리다': 55,\n",
       " '성능': 124,\n",
       " '많이': 75,\n",
       " '구매': 24,\n",
       " '만족': 71,\n",
       " '엄청': 158,\n",
       " '보다': 96,\n",
       " '수준': 129,\n",
       " '민감하다': 86,\n",
       " '조금': 192,\n",
       " '문제': 85,\n",
       " '그리고': 28,\n",
       " '발생': 90,\n",
       " '무게': 82,\n",
       " '때문': 66,\n",
       " '제품': 191,\n",
       " '굉장하다': 23,\n",
       " '좋다': 195,\n",
       " '만족하다': 73,\n",
       " '걱정': 15,\n",
       " '상태': 118,\n",
       " '가볍다': 7,\n",
       " '커지다': 207,\n",
       " '마감': 68,\n",
       " '시끄럽다': 132,\n",
       " 'ssd': 2,\n",
       " '사운드': 113,\n",
       " '별로': 94,\n",
       " '주다': 196,\n",
       " '속도': 128,\n",
       " '느리다': 43,\n",
       " '되어다': 58,\n",
       " '엄청나다': 159,\n",
       " '해주다': 222,\n",
       " '보통': 100,\n",
       " '계속': 19,\n",
       " '독서실': 54,\n",
       " '서비스': 121,\n",
       " '확실하다': 226,\n",
       " '다른': 45,\n",
       " '추천': 204,\n",
       " '받다': 89,\n",
       " '부팅': 102,\n",
       " '쓰다': 141,\n",
       " '그렇게': 26,\n",
       " '마음': 70,\n",
       " 'as': 1,\n",
       " '다니다': 44,\n",
       " '고민': 21,\n",
       " '모르다': 80,\n",
       " '어쩔': 157,\n",
       " '거의': 14,\n",
       " '보시': 98,\n",
       " '잡다': 183,\n",
       " '그래도': 25,\n",
       " '신경': 133,\n",
       " '어느': 156,\n",
       " '가능하다': 5,\n",
       " '다소': 46,\n",
       " '매우': 76,\n",
       " '스피커': 130,\n",
       " '더욱': 50,\n",
       " '잘쓰다': 182,\n",
       " '딱하다': 65,\n",
       " '쓰이다': 142,\n",
       " '안나': 149,\n",
       " '고려': 20,\n",
       " '조절': 194,\n",
       " '만족스럽다': 72,\n",
       " '느끼다': 41,\n",
       " '가끔': 4,\n",
       " '자체': 179,\n",
       " '기본': 31,\n",
       " '하나': 217,\n",
       " '아쉽다': 145,\n",
       " '빠르다': 106,\n",
       " '화면': 224,\n",
       " '하드': 219,\n",
       " '제법': 190,\n",
       " '음부': 170,\n",
       " '비교': 104,\n",
       " '차다': 200,\n",
       " '느껴지다': 40,\n",
       " '훌륭하다': 228,\n",
       " '가성': 8,\n",
       " '적응': 186,\n",
       " '안되다': 150,\n",
       " '개선': 12,\n",
       " '근데': 30,\n",
       " '보드': 97,\n",
       " '생기다': 120,\n",
       " '데스크탑': 51,\n",
       " '비다': 105,\n",
       " '오래': 165,\n",
       " '가다': 6,\n",
       " '설치': 123,\n",
       " '싸다': 138,\n",
       " '드리다': 60,\n",
       " '무척': 84,\n",
       " '모든': 79,\n",
       " '완벽하다': 166,\n",
       " '보고': 95,\n",
       " '갑자기': 10,\n",
       " '삼성': 116,\n",
       " '아예': 146,\n",
       " '최고': 202,\n",
       " '나서다': 35,\n",
       " '그리다': 29,\n",
       " '디자인': 64,\n",
       " '쿨러': 209,\n",
       " '역시': 163,\n",
       " '살짝': 115,\n",
       " '장점': 184,\n",
       " '써다': 139,\n",
       " '시간': 131,\n",
       " '듭니': 63,\n",
       " '대다': 48,\n",
       " '무겁다': 81,\n",
       " '확인': 227,\n",
       " '전혀': 187,\n",
       " '감다': 9,\n",
       " '적다': 185,\n",
       " '무엇': 83,\n",
       " '착하다': 201,\n",
       " '진짜': 199,\n",
       " '완전': 167,\n",
       " '10': 0,\n",
       " '휴대': 230,\n",
       " '작고': 180,\n",
       " '도적': 53,\n",
       " '깔끔하다': 33,\n",
       " '모두': 78,\n",
       " '이쁘다': 173,\n",
       " '없이': 162,\n",
       " '쓰기': 140,\n",
       " '편하다': 216,\n",
       " '화질': 225,\n",
       " '또한': 67,\n",
       " '우수하다': 168,\n",
       " '작다': 181,\n",
       " '지금': 198,\n",
       " '느낌': 42,\n",
       " '불편하다': 103,\n",
       " '배송': 92,\n",
       " '선택': 122,\n",
       " '쾌적하다': 208,\n",
       " '빠릿빠릿': 107,\n",
       " '일단': 176,\n",
       " '추가': 203,\n",
       " '훨씬': 229,\n",
       " '모델': 77,\n",
       " '양품': 154,\n",
       " '오다': 164,\n",
       " '양호': 155,\n",
       " '아직': 148,\n",
       " '크기': 211,\n",
       " '준수': 197,\n",
       " '기존': 32,\n",
       " '배터리': 93,\n",
       " '사은': 114,\n",
       " '빨르다': 108,\n",
       " '낮다': 36,\n",
       " '마우스': 69,\n",
       " '드네': 59,\n",
       " '이상': 174}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(cnv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label, stratify = label)\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(train_data).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(train_label).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([821]), torch.Size([821, 231]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size(), x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data.TensorDataset(x, y)\n",
    "batch_size = 10\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size = batch_size, shuffle=True, num_workers=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = nn.Linear(dim, 200, bias=True)\n",
    "linear2 = nn.Linear(200, 3, bias=True)\n",
    "\n",
    "relu = nn.ReLU()\n",
    "\n",
    "model = nn.Sequential(linear1, relu, linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], lter [20/82] Loss: 0.9834\n",
      "Epoch [1/50], lter [40/82] Loss: 0.8389\n",
      "Epoch [1/50], lter [60/82] Loss: 0.6295\n",
      "Epoch [1/50], lter [80/82] Loss: 0.4904\n",
      "Epoch [2/50], lter [20/82] Loss: 0.6149\n",
      "Epoch [2/50], lter [40/82] Loss: 0.4172\n",
      "Epoch [2/50], lter [60/82] Loss: 0.4971\n",
      "Epoch [2/50], lter [80/82] Loss: 0.2605\n",
      "Epoch [3/50], lter [20/82] Loss: 0.3513\n",
      "Epoch [3/50], lter [40/82] Loss: 0.4924\n",
      "Epoch [3/50], lter [60/82] Loss: 0.3678\n",
      "Epoch [3/50], lter [80/82] Loss: 0.2578\n",
      "Epoch [4/50], lter [20/82] Loss: 0.2885\n",
      "Epoch [4/50], lter [40/82] Loss: 0.1850\n",
      "Epoch [4/50], lter [60/82] Loss: 0.2613\n",
      "Epoch [4/50], lter [80/82] Loss: 0.2603\n",
      "Epoch [5/50], lter [20/82] Loss: 0.1040\n",
      "Epoch [5/50], lter [40/82] Loss: 0.1731\n",
      "Epoch [5/50], lter [60/82] Loss: 0.1968\n",
      "Epoch [5/50], lter [80/82] Loss: 0.4155\n",
      "Epoch [6/50], lter [20/82] Loss: 0.0968\n",
      "Epoch [6/50], lter [40/82] Loss: 0.2398\n",
      "Epoch [6/50], lter [60/82] Loss: 0.2277\n",
      "Epoch [6/50], lter [80/82] Loss: 0.1422\n",
      "Epoch [7/50], lter [20/82] Loss: 0.1702\n",
      "Epoch [7/50], lter [40/82] Loss: 0.2289\n",
      "Epoch [7/50], lter [60/82] Loss: 0.0539\n",
      "Epoch [7/50], lter [80/82] Loss: 0.1078\n",
      "Epoch [8/50], lter [20/82] Loss: 0.5307\n",
      "Epoch [8/50], lter [40/82] Loss: 0.1355\n",
      "Epoch [8/50], lter [60/82] Loss: 0.2658\n",
      "Epoch [8/50], lter [80/82] Loss: 0.2094\n",
      "Epoch [9/50], lter [20/82] Loss: 0.0396\n",
      "Epoch [9/50], lter [40/82] Loss: 0.2028\n",
      "Epoch [9/50], lter [60/82] Loss: 0.1006\n",
      "Epoch [9/50], lter [80/82] Loss: 0.3328\n",
      "Epoch [10/50], lter [20/82] Loss: 0.1833\n",
      "Epoch [10/50], lter [40/82] Loss: 0.0419\n",
      "Epoch [10/50], lter [60/82] Loss: 0.0673\n",
      "Epoch [10/50], lter [80/82] Loss: 0.0416\n",
      "Epoch [11/50], lter [20/82] Loss: 0.0559\n",
      "Epoch [11/50], lter [40/82] Loss: 0.6521\n",
      "Epoch [11/50], lter [60/82] Loss: 0.0639\n",
      "Epoch [11/50], lter [80/82] Loss: 0.0181\n",
      "Epoch [12/50], lter [20/82] Loss: 0.2075\n",
      "Epoch [12/50], lter [40/82] Loss: 0.0163\n",
      "Epoch [12/50], lter [60/82] Loss: 0.0720\n",
      "Epoch [12/50], lter [80/82] Loss: 0.0330\n",
      "Epoch [13/50], lter [20/82] Loss: 0.2180\n",
      "Epoch [13/50], lter [40/82] Loss: 0.1217\n",
      "Epoch [13/50], lter [60/82] Loss: 0.0420\n",
      "Epoch [13/50], lter [80/82] Loss: 0.1082\n",
      "Epoch [14/50], lter [20/82] Loss: 0.0796\n",
      "Epoch [14/50], lter [40/82] Loss: 0.0464\n",
      "Epoch [14/50], lter [60/82] Loss: 0.2400\n",
      "Epoch [14/50], lter [80/82] Loss: 0.0573\n",
      "Epoch [15/50], lter [20/82] Loss: 0.2361\n",
      "Epoch [15/50], lter [40/82] Loss: 0.0609\n",
      "Epoch [15/50], lter [60/82] Loss: 0.0120\n",
      "Epoch [15/50], lter [80/82] Loss: 0.1506\n",
      "Epoch [16/50], lter [20/82] Loss: 0.0131\n",
      "Epoch [16/50], lter [40/82] Loss: 0.1308\n",
      "Epoch [16/50], lter [60/82] Loss: 0.1288\n",
      "Epoch [16/50], lter [80/82] Loss: 0.1296\n",
      "Epoch [17/50], lter [20/82] Loss: 0.0703\n",
      "Epoch [17/50], lter [40/82] Loss: 0.0155\n",
      "Epoch [17/50], lter [60/82] Loss: 0.0049\n",
      "Epoch [17/50], lter [80/82] Loss: 0.2189\n",
      "Epoch [18/50], lter [20/82] Loss: 0.1061\n",
      "Epoch [18/50], lter [40/82] Loss: 0.0351\n",
      "Epoch [18/50], lter [60/82] Loss: 0.0169\n",
      "Epoch [18/50], lter [80/82] Loss: 0.0068\n",
      "Epoch [19/50], lter [20/82] Loss: 0.0102\n",
      "Epoch [19/50], lter [40/82] Loss: 0.0506\n",
      "Epoch [19/50], lter [60/82] Loss: 0.0142\n",
      "Epoch [19/50], lter [80/82] Loss: 0.0090\n",
      "Epoch [20/50], lter [20/82] Loss: 0.0034\n",
      "Epoch [20/50], lter [40/82] Loss: 0.0522\n",
      "Epoch [20/50], lter [60/82] Loss: 0.0053\n",
      "Epoch [20/50], lter [80/82] Loss: 0.0147\n",
      "Epoch [21/50], lter [20/82] Loss: 0.1314\n",
      "Epoch [21/50], lter [40/82] Loss: 0.0021\n",
      "Epoch [21/50], lter [60/82] Loss: 0.0983\n",
      "Epoch [21/50], lter [80/82] Loss: 0.1392\n",
      "Epoch [22/50], lter [20/82] Loss: 0.0068\n",
      "Epoch [22/50], lter [40/82] Loss: 0.0216\n",
      "Epoch [22/50], lter [60/82] Loss: 0.0041\n",
      "Epoch [22/50], lter [80/82] Loss: 0.0283\n",
      "Epoch [23/50], lter [20/82] Loss: 0.0041\n",
      "Epoch [23/50], lter [40/82] Loss: 0.0582\n",
      "Epoch [23/50], lter [60/82] Loss: 0.0642\n",
      "Epoch [23/50], lter [80/82] Loss: 0.0268\n",
      "Epoch [24/50], lter [20/82] Loss: 0.0055\n",
      "Epoch [24/50], lter [40/82] Loss: 0.0258\n",
      "Epoch [24/50], lter [60/82] Loss: 0.0081\n",
      "Epoch [24/50], lter [80/82] Loss: 0.0046\n",
      "Epoch [25/50], lter [20/82] Loss: 0.0061\n",
      "Epoch [25/50], lter [40/82] Loss: 0.1411\n",
      "Epoch [25/50], lter [60/82] Loss: 0.0775\n",
      "Epoch [25/50], lter [80/82] Loss: 0.4390\n",
      "Epoch [26/50], lter [20/82] Loss: 0.0108\n",
      "Epoch [26/50], lter [40/82] Loss: 0.1359\n",
      "Epoch [26/50], lter [60/82] Loss: 0.1792\n",
      "Epoch [26/50], lter [80/82] Loss: 0.0983\n",
      "Epoch [27/50], lter [20/82] Loss: 0.0082\n",
      "Epoch [27/50], lter [40/82] Loss: 0.0234\n",
      "Epoch [27/50], lter [60/82] Loss: 0.0073\n",
      "Epoch [27/50], lter [80/82] Loss: 0.0906\n",
      "Epoch [28/50], lter [20/82] Loss: 0.0093\n",
      "Epoch [28/50], lter [40/82] Loss: 0.0102\n",
      "Epoch [28/50], lter [60/82] Loss: 0.4250\n",
      "Epoch [28/50], lter [80/82] Loss: 0.0629\n",
      "Epoch [29/50], lter [20/82] Loss: 0.0085\n",
      "Epoch [29/50], lter [40/82] Loss: 0.0248\n",
      "Epoch [29/50], lter [60/82] Loss: 0.0148\n",
      "Epoch [29/50], lter [80/82] Loss: 0.3588\n",
      "Epoch [30/50], lter [20/82] Loss: 0.0041\n",
      "Epoch [30/50], lter [40/82] Loss: 0.0094\n",
      "Epoch [30/50], lter [60/82] Loss: 0.0738\n",
      "Epoch [30/50], lter [80/82] Loss: 0.0524\n",
      "Epoch [31/50], lter [20/82] Loss: 0.0020\n",
      "Epoch [31/50], lter [40/82] Loss: 0.0066\n",
      "Epoch [31/50], lter [60/82] Loss: 0.0066\n",
      "Epoch [31/50], lter [80/82] Loss: 0.0980\n",
      "Epoch [32/50], lter [20/82] Loss: 0.0016\n",
      "Epoch [32/50], lter [40/82] Loss: 0.0147\n",
      "Epoch [32/50], lter [60/82] Loss: 0.0078\n",
      "Epoch [32/50], lter [80/82] Loss: 0.0216\n",
      "Epoch [33/50], lter [20/82] Loss: 0.0793\n",
      "Epoch [33/50], lter [40/82] Loss: 0.0017\n",
      "Epoch [33/50], lter [60/82] Loss: 0.0041\n",
      "Epoch [33/50], lter [80/82] Loss: 0.0037\n",
      "Epoch [34/50], lter [20/82] Loss: 0.0069\n",
      "Epoch [34/50], lter [40/82] Loss: 0.0115\n",
      "Epoch [34/50], lter [60/82] Loss: 0.1105\n",
      "Epoch [34/50], lter [80/82] Loss: 0.0142\n",
      "Epoch [35/50], lter [20/82] Loss: 0.0023\n",
      "Epoch [35/50], lter [40/82] Loss: 0.0035\n",
      "Epoch [35/50], lter [60/82] Loss: 0.0248\n",
      "Epoch [35/50], lter [80/82] Loss: 0.2925\n",
      "Epoch [36/50], lter [20/82] Loss: 0.0023\n",
      "Epoch [36/50], lter [40/82] Loss: 0.0010\n",
      "Epoch [36/50], lter [60/82] Loss: 0.0638\n",
      "Epoch [36/50], lter [80/82] Loss: 0.0006\n",
      "Epoch [37/50], lter [20/82] Loss: 0.0819\n",
      "Epoch [37/50], lter [40/82] Loss: 0.0117\n",
      "Epoch [37/50], lter [60/82] Loss: 0.0075\n",
      "Epoch [37/50], lter [80/82] Loss: 0.0152\n",
      "Epoch [38/50], lter [20/82] Loss: 0.1072\n",
      "Epoch [38/50], lter [40/82] Loss: 0.0038\n",
      "Epoch [38/50], lter [60/82] Loss: 0.0058\n",
      "Epoch [38/50], lter [80/82] Loss: 0.0048\n",
      "Epoch [39/50], lter [20/82] Loss: 0.2533\n",
      "Epoch [39/50], lter [40/82] Loss: 0.0571\n",
      "Epoch [39/50], lter [60/82] Loss: 0.0062\n",
      "Epoch [39/50], lter [80/82] Loss: 0.0032\n",
      "Epoch [40/50], lter [20/82] Loss: 0.0083\n",
      "Epoch [40/50], lter [40/82] Loss: 0.1075\n",
      "Epoch [40/50], lter [60/82] Loss: 0.0083\n",
      "Epoch [40/50], lter [80/82] Loss: 0.0034\n",
      "Epoch [41/50], lter [20/82] Loss: 0.2311\n",
      "Epoch [41/50], lter [40/82] Loss: 0.0013\n",
      "Epoch [41/50], lter [60/82] Loss: 0.0171\n",
      "Epoch [41/50], lter [80/82] Loss: 0.0076\n",
      "Epoch [42/50], lter [20/82] Loss: 0.0747\n",
      "Epoch [42/50], lter [40/82] Loss: 0.0005\n",
      "Epoch [42/50], lter [60/82] Loss: 0.0018\n",
      "Epoch [42/50], lter [80/82] Loss: 0.0496\n",
      "Epoch [43/50], lter [20/82] Loss: 0.1440\n",
      "Epoch [43/50], lter [40/82] Loss: 0.0322\n",
      "Epoch [43/50], lter [60/82] Loss: 0.2729\n",
      "Epoch [43/50], lter [80/82] Loss: 0.0280\n",
      "Epoch [44/50], lter [20/82] Loss: 0.0899\n",
      "Epoch [44/50], lter [40/82] Loss: 0.0024\n",
      "Epoch [44/50], lter [60/82] Loss: 0.0009\n",
      "Epoch [44/50], lter [80/82] Loss: 0.0144\n",
      "Epoch [45/50], lter [20/82] Loss: 0.0516\n",
      "Epoch [45/50], lter [40/82] Loss: 0.0008\n",
      "Epoch [45/50], lter [60/82] Loss: 0.0175\n",
      "Epoch [45/50], lter [80/82] Loss: 0.0916\n",
      "Epoch [46/50], lter [20/82] Loss: 0.0326\n",
      "Epoch [46/50], lter [40/82] Loss: 0.3764\n",
      "Epoch [46/50], lter [60/82] Loss: 0.0026\n",
      "Epoch [46/50], lter [80/82] Loss: 0.0007\n",
      "Epoch [47/50], lter [20/82] Loss: 0.0037\n",
      "Epoch [47/50], lter [40/82] Loss: 0.0343\n",
      "Epoch [47/50], lter [60/82] Loss: 0.0079\n",
      "Epoch [47/50], lter [80/82] Loss: 0.1370\n",
      "Epoch [48/50], lter [20/82] Loss: 0.0003\n",
      "Epoch [48/50], lter [40/82] Loss: 0.0007\n",
      "Epoch [48/50], lter [60/82] Loss: 0.0006\n",
      "Epoch [48/50], lter [80/82] Loss: 0.0009\n",
      "Epoch [49/50], lter [20/82] Loss: 0.0105\n",
      "Epoch [49/50], lter [40/82] Loss: 0.1374\n",
      "Epoch [49/50], lter [60/82] Loss: 0.0368\n",
      "Epoch [49/50], lter [80/82] Loss: 0.0089\n",
      "Epoch [50/50], lter [20/82] Loss: 0.0663\n",
      "Epoch [50/50], lter [40/82] Loss: 0.0413\n",
      "Epoch [50/50], lter [60/82] Loss: 0.0650\n",
      "Epoch [50/50], lter [80/82] Loss: 0.0416\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, (batch_text, batch_labels) in enumerate(train_loader):\n",
    "        X = batch_text.view(-1, dim)\n",
    "        Y = batch_labels\n",
    "        \n",
    "        pre = model(X)\n",
    "        cost = loss(pre, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "             print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "    \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "test_data = Data.TensorDataset(x_test, y_test)\n",
    "\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test text: 81.751825 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for text, labels in test_loader:\n",
    "    text = text.view(-1, dim)\n",
    "    outputs = model(text)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += 1\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print(\"Accuracy of test text: %f %%\" % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsitNoisy(string):\n",
    "    results = []\n",
    "    tokens = twitter.pos(string, norm=True, stem=True)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "            \n",
    "    sample = cnv.transform([\" \".join(results).strip()]).toarray()\n",
    "    sample = torch.from_numpy(sample).type(torch.FloatTensor)\n",
    "    outputs = model(sample)\n",
    "    pre = torch.max(outputs.data, 1)[1].numpy()\n",
    "    \n",
    "    if pre == 0:\n",
    "        print(\"분석결과: 소음 거의 없음\")\n",
    "    elif pre == 1:\n",
    "        print(\"분석결과: 소음 조금 있음\")\n",
    "    else:\n",
    "        print(\"분석결과: 소음 매우 심함\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석결과: 소음 조금 있음\n"
     ]
    }
   ],
   "source": [
    "IsitNoisy(\"소리가 조금 거슬리더라고요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석결과: 소음 거의 없음\n"
     ]
    }
   ],
   "source": [
    "IsitNoisy(\"아주 조용합니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석결과: 소음 매우 심함\n"
     ]
    }
   ],
   "source": [
    "IsitNoisy(\"너무 시끄러워요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
