{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/score_발열.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "data = pd.read_excel(filename, sheet_name=sheet_name, header=0)\n",
    "\n",
    "csv_data = [item.replace('#', '').strip() for item in data['Review']]\n",
    "csv_label = data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['발열히 심한거 같은데 여름이라 그런가?..',\n",
       " '발열이좀 심한거 같아서 걱정이에요',\n",
       " '발열이심하더라구요',\n",
       " '발열이너무심한게 제일큰 단점인것 같고 그외에 불편한점은',\n",
       " '발열이...정말...심합니다']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Okt()\n",
    "size = 500\n",
    "\n",
    "doc = []\n",
    "\n",
    "for sentence in csv_data:\n",
    "    results = []\n",
    "    tokens = twitter.pos(sentence, norm=True, stem=True)\n",
    "\n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "    doc.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['발열', '히', '심하다', '같다', '여름', '그', '런가'],\n",
       " ['발열', '이', '좀', '심하다', '같다', '걱정'],\n",
       " ['발열', '심하다'],\n",
       " ['발열', '이', '너', '무심하다', '제일', '크다', '단점', '것', '같다', '그', '외', '불편하다', '점'],\n",
       " ['발열', '정말', '심하다']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(doc, size=size, window=2, hs=0, min_count = 3, sg=0)\n",
    "\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['발열', '없다', '있다', '하다', '소음', '좋다', '심하다', '같다', '않다', '거의', '좀', '적다', '자다', '것', '잡다', '문제', '조금', '되다', '정도', '생각', '만족하다', '괜찮다', '못', '노트북', '사용', '이', '느끼다', '잘', '너무', '부분', '이다', '팬', '성능', '쿨러', '정말', '돌아가다', '제품', '별로', '걱정', '크다', '배터리', '수', '많이', '더', '전혀', '게임', '보다', '속도', '도', '아직', '매우', '아주', '가볍다', '소리', '안', '쓸다', '어쩔', '느껴지다', '다', '때문', '적', '나다', '그렇다', '키', '편이', '관리', '아니다', '신경', '크게', '조용하다', '약간', '때', '많다', '하', '및', '네', '보이다', '만족', '그', '쓰다', '상당하다', '들다', '없이', '거', '꽤', '보드', '오래', '쿨링', '무게', '아쉽다', '디자인', '제어', '점', '심해', '높다', '빠르다', '양호', '또한', '대', '모두', '빼다', '돌리다', '모르다', '발생', '다른', '맘', '감다', '잡히다', '되어다', '확실하다', '요', '진짜', '들', '삼성', '시간', '해보다', '사양', '가격', '감', '편', '모델', '안나', '가다', '비', '개선', '단점', '사', '느낌', '하지만', '제', '뜨겁다', '펜', '하나', '시', '듯', '사은', '써다', '수준', '최고', '만족스럽다', '은', '구매', '습', '메탈', '약하다', '상태', '충전', '판', '안되다', '굉장하다', '훨씬', '품', '작업', '화면', '발', '열량', '그렇게', '장시간', '생기다', '지다', '중', '무엇', '걸리다', '받침', '구성', '깔끔하다', '저', '인터넷', '그래픽', '패드', '이상', '그램', '성', '한', '아', '기능', '불편하다', '프로그램', '심', '심다', '마음', '듭니', '막', '부팅', '느리다', '센터', '늘다', '3', '낮다', '지금', '빠지다', '준수', '처리', '알', '주다', '어느', '배송', '그래도', '필요', '돌다', '아예', '데스크탑', '따뜻하다', '카드', '고', '엄청', '완벽하다', '오다', '면', '가성', '우수하다', '선택', '덜', '작다', '패널', '히', '램', '용량', '끊기다', '이렇다', '원래', '살짝', '나', '화이트', '내다', '인하다', '고민', '내', '이쁘다', '현상', '장점', '일어나서', '편입', '니', '컴퓨터', '이정', '따다', '편하다', '이면', '받다', '뭐', '나오다', '등등', '후', '뜨다', '사다', '설치', '50', '70', '쓰이다', '펴다', 'ㅜㅜ', '등', '모니터', '던지다', '추천', '그리다', '대다', '대비', '딱하다', '뒤틀리다', '모든', '완전', '알다', '뛰어나다', '특히', '기', '물론', '역시', '이슈', '여름', '너', '무심하다', '외', '한편', '밧데리', '자판', '이전', '불량', '심해지다', '라면', '화상', '신분', '야하다', '제외', '자주', '5', '치다', '소', '어', '전', '하드', '왼쪽', '비다', '비추다', '휴대', '다니다', '9', '이제', '조절', '잡지', '다소', '따르다', '4', '차이', '수가', '어떻다', '켜다', '능력', '떨어지다', '오래되다', '손', '임', '옆', '돼다', '그냥', '절대', '한성', '얇다', '일이', '해소', '쫌', '기본', 'S', '서비스', '정', '문서', '두다', '온도', '예쁘다', '뜻', '겨울', '나쁘다', '액정', '레노버', '처음', '얘기', '연결', 'ㅎ', '평소', '노트', '곳', '별', 'SSD', '마감', '윈도우', '7', '추가', '보통', '분', '차다', '빠릿빠릿', '일', '적당하다', '굿', '무겁다', '줄', 'i', '짧다', '기다', '딱', '증상', '현재', '화질', '보', 'ssd', '하니', '이렇게', '화', '상', '금', '놀라다', '점수', '무리', '외관', '대단하다', '저렴하다', '동', '잡고', '엄청나다', '안심', '배그', 'ips', '할인'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = []\n",
    "max_length = 0\n",
    "\n",
    "for sentence in doc:\n",
    "    temp = []\n",
    "    length = 0\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in w2v.keys():\n",
    "            temp.append(w2v[word])\n",
    "            length += 1\n",
    "    doc2vec.append(temp)\n",
    "    \n",
    "    if max_length <= length:\n",
    "        max_length = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc2vec:\n",
    "    length = len(sentence)\n",
    "    \n",
    "    while length < max_length:\n",
    "        sentence.append(np.zeros(size))\n",
    "        length += 1\n",
    "\n",
    "doc2vec = np.array(doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.60265770e-02,  5.03251748e-03,  2.23066099e-02, ...,\n",
       "          6.12424174e-03,  6.98140776e-03,  2.67418269e-02],\n",
       "        [-1.97218289e-03,  5.79213200e-04,  2.39920197e-03, ...,\n",
       "          7.89380923e-04, -8.01704009e-05,  2.63706618e-03],\n",
       "        [-4.46797255e-03,  1.79528794e-03,  6.11944683e-03, ...,\n",
       "          2.59042811e-03,  2.68662092e-03,  6.86611468e-03],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-1.60265770e-02,  5.03251748e-03,  2.23066099e-02, ...,\n",
       "          6.12424174e-03,  6.98140776e-03,  2.67418269e-02],\n",
       "        [-5.93850017e-03,  8.62687302e-04,  7.74806459e-03, ...,\n",
       "          3.07461247e-03,  2.85412045e-03,  8.50566477e-03],\n",
       "        [-2.86587165e-03,  1.15589995e-03,  6.46650838e-03, ...,\n",
       "          1.52650964e-03,  1.72232313e-03,  7.27579603e-03],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-1.60265770e-02,  5.03251748e-03,  2.23066099e-02, ...,\n",
       "          6.12424174e-03,  6.98140776e-03,  2.67418269e-02],\n",
       "        [-4.46797255e-03,  1.79528794e-03,  6.11944683e-03, ...,\n",
       "          2.59042811e-03,  2.68662092e-03,  6.86611468e-03],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.60265770e-02,  5.03251748e-03,  2.23066099e-02, ...,\n",
       "          6.12424174e-03,  6.98140776e-03,  2.67418269e-02],\n",
       "        [-3.11240531e-03,  1.22352038e-03,  4.86698886e-03, ...,\n",
       "          9.45750508e-04,  1.74295332e-03,  6.52183779e-03],\n",
       "        [-5.61256194e-03,  1.18526886e-03,  8.48148298e-03, ...,\n",
       "          2.67472863e-03,  2.94077420e-03,  9.58620757e-03],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-1.60265770e-02,  5.03251748e-03,  2.23066099e-02, ...,\n",
       "          6.12424174e-03,  6.98140776e-03,  2.67418269e-02],\n",
       "        [-8.66475515e-03,  1.95037038e-03,  1.32526997e-02, ...,\n",
       "          4.37895302e-03,  3.68512631e-03,  1.51540181e-02],\n",
       "        [-4.53451276e-03,  2.02945154e-03,  7.38464575e-03, ...,\n",
       "          1.41549786e-03,  3.26978578e-03,  8.16910155e-03],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-1.60265770e-02,  5.03251748e-03,  2.23066099e-02, ...,\n",
       "          6.12424174e-03,  6.98140776e-03,  2.67418269e-02],\n",
       "        [-4.46797255e-03,  1.79528794e-03,  6.11944683e-03, ...,\n",
       "          2.59042811e-03,  2.68662092e-03,  6.86611468e-03],\n",
       "        [-6.87056873e-03,  1.13239058e-03,  8.23718496e-03, ...,\n",
       "          2.58900714e-03,  2.65005161e-03,  1.16292369e-02],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211, 12, 500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = doc2vec\n",
    "label = csv_label.values\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(train_data).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(train_label).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([908, 12, 500]), torch.Size([908]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([908, 1, 12, 500])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(-1, 1, 12, 500)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data.TensorDataset(x, y)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size = batch_size,\n",
    "                              shuffle=True, num_workers=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 12, 500])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, label = iter(train_loader).next()\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3), # 1*12*500 -> 16 * 10 * 498\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3), # 16 * 10 * 498 -> 32 * 8 * 496\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 32 * 8 * 496 -> 32 * 4 * 248\n",
    "            nn.Conv2d(32, 64, 3), # 32 * 4 * 248 -> 64 * 2 * 246\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) # 64 * 2 * 246 -> 64 * 1 * 123\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64 * 1 * 123, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(-1, 64 * 1 * 123)\n",
    "        out = self.fc_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "# if cuda -> model = CNN().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], lter [20/90], Loss: 0.9991\n",
      "Epoch [1/50], lter [40/90], Loss: 0.8754\n",
      "Epoch [1/50], lter [60/90], Loss: 0.7098\n",
      "Epoch [1/50], lter [80/90], Loss: 1.0356\n",
      "Epoch [2/50], lter [20/90], Loss: 1.0631\n",
      "Epoch [2/50], lter [40/90], Loss: 0.9328\n",
      "Epoch [2/50], lter [60/90], Loss: 1.1051\n",
      "Epoch [2/50], lter [80/90], Loss: 0.9744\n",
      "Epoch [3/50], lter [20/90], Loss: 0.9786\n",
      "Epoch [3/50], lter [40/90], Loss: 1.0117\n",
      "Epoch [3/50], lter [60/90], Loss: 0.8976\n",
      "Epoch [3/50], lter [80/90], Loss: 0.6685\n",
      "Epoch [4/50], lter [20/90], Loss: 0.8985\n",
      "Epoch [4/50], lter [40/90], Loss: 0.9027\n",
      "Epoch [4/50], lter [60/90], Loss: 0.8532\n",
      "Epoch [4/50], lter [80/90], Loss: 0.7124\n",
      "Epoch [5/50], lter [20/90], Loss: 0.7895\n",
      "Epoch [5/50], lter [40/90], Loss: 0.9662\n",
      "Epoch [5/50], lter [60/90], Loss: 1.0996\n",
      "Epoch [5/50], lter [80/90], Loss: 0.9959\n",
      "Epoch [6/50], lter [20/90], Loss: 0.7937\n",
      "Epoch [6/50], lter [40/90], Loss: 0.7738\n",
      "Epoch [6/50], lter [60/90], Loss: 0.6531\n",
      "Epoch [6/50], lter [80/90], Loss: 0.8387\n",
      "Epoch [7/50], lter [20/90], Loss: 0.7465\n",
      "Epoch [7/50], lter [40/90], Loss: 0.9869\n",
      "Epoch [7/50], lter [60/90], Loss: 0.8862\n",
      "Epoch [7/50], lter [80/90], Loss: 0.8318\n",
      "Epoch [8/50], lter [20/90], Loss: 1.3950\n",
      "Epoch [8/50], lter [40/90], Loss: 0.9017\n",
      "Epoch [8/50], lter [60/90], Loss: 0.6581\n",
      "Epoch [8/50], lter [80/90], Loss: 0.9011\n",
      "Epoch [9/50], lter [20/90], Loss: 0.8379\n",
      "Epoch [9/50], lter [40/90], Loss: 1.0351\n",
      "Epoch [9/50], lter [60/90], Loss: 0.7955\n",
      "Epoch [9/50], lter [80/90], Loss: 1.0043\n",
      "Epoch [10/50], lter [20/90], Loss: 0.6453\n",
      "Epoch [10/50], lter [40/90], Loss: 1.2533\n",
      "Epoch [10/50], lter [60/90], Loss: 1.4818\n",
      "Epoch [10/50], lter [80/90], Loss: 0.7029\n",
      "Epoch [11/50], lter [20/90], Loss: 1.1446\n",
      "Epoch [11/50], lter [40/90], Loss: 0.7140\n",
      "Epoch [11/50], lter [60/90], Loss: 0.7147\n",
      "Epoch [11/50], lter [80/90], Loss: 0.9950\n",
      "Epoch [12/50], lter [20/90], Loss: 0.9012\n",
      "Epoch [12/50], lter [40/90], Loss: 1.0115\n",
      "Epoch [12/50], lter [60/90], Loss: 0.6892\n",
      "Epoch [12/50], lter [80/90], Loss: 0.9353\n",
      "Epoch [13/50], lter [20/90], Loss: 0.9718\n",
      "Epoch [13/50], lter [40/90], Loss: 0.7256\n",
      "Epoch [13/50], lter [60/90], Loss: 1.0148\n",
      "Epoch [13/50], lter [80/90], Loss: 0.7659\n",
      "Epoch [14/50], lter [20/90], Loss: 0.5936\n",
      "Epoch [14/50], lter [40/90], Loss: 0.7916\n",
      "Epoch [14/50], lter [60/90], Loss: 0.8068\n",
      "Epoch [14/50], lter [80/90], Loss: 0.7281\n",
      "Epoch [15/50], lter [20/90], Loss: 1.2190\n",
      "Epoch [15/50], lter [40/90], Loss: 0.8998\n",
      "Epoch [15/50], lter [60/90], Loss: 0.8985\n",
      "Epoch [15/50], lter [80/90], Loss: 0.9291\n",
      "Epoch [16/50], lter [20/90], Loss: 0.7751\n",
      "Epoch [16/50], lter [40/90], Loss: 0.7388\n",
      "Epoch [16/50], lter [60/90], Loss: 1.0509\n",
      "Epoch [16/50], lter [80/90], Loss: 0.9730\n",
      "Epoch [17/50], lter [20/90], Loss: 0.9806\n",
      "Epoch [17/50], lter [40/90], Loss: 1.1993\n",
      "Epoch [17/50], lter [60/90], Loss: 0.7686\n",
      "Epoch [17/50], lter [80/90], Loss: 0.4759\n",
      "Epoch [18/50], lter [20/90], Loss: 0.7264\n",
      "Epoch [18/50], lter [40/90], Loss: 0.7285\n",
      "Epoch [18/50], lter [60/90], Loss: 1.1606\n",
      "Epoch [18/50], lter [80/90], Loss: 0.8230\n",
      "Epoch [19/50], lter [20/90], Loss: 1.2476\n",
      "Epoch [19/50], lter [40/90], Loss: 0.9597\n",
      "Epoch [19/50], lter [60/90], Loss: 0.9954\n",
      "Epoch [19/50], lter [80/90], Loss: 0.9625\n",
      "Epoch [20/50], lter [20/90], Loss: 0.9344\n",
      "Epoch [20/50], lter [40/90], Loss: 0.6855\n",
      "Epoch [20/50], lter [60/90], Loss: 0.7614\n",
      "Epoch [20/50], lter [80/90], Loss: 0.8047\n",
      "Epoch [21/50], lter [20/90], Loss: 1.2726\n",
      "Epoch [21/50], lter [40/90], Loss: 0.9620\n",
      "Epoch [21/50], lter [60/90], Loss: 1.0690\n",
      "Epoch [21/50], lter [80/90], Loss: 1.4491\n",
      "Epoch [22/50], lter [20/90], Loss: 0.9213\n",
      "Epoch [22/50], lter [40/90], Loss: 1.0635\n",
      "Epoch [22/50], lter [60/90], Loss: 1.0448\n",
      "Epoch [22/50], lter [80/90], Loss: 0.8109\n",
      "Epoch [23/50], lter [20/90], Loss: 0.8326\n",
      "Epoch [23/50], lter [40/90], Loss: 0.8684\n",
      "Epoch [23/50], lter [60/90], Loss: 1.2021\n",
      "Epoch [23/50], lter [80/90], Loss: 0.8129\n",
      "Epoch [24/50], lter [20/90], Loss: 0.6377\n",
      "Epoch [24/50], lter [40/90], Loss: 0.9648\n",
      "Epoch [24/50], lter [60/90], Loss: 1.1368\n",
      "Epoch [24/50], lter [80/90], Loss: 1.1333\n",
      "Epoch [25/50], lter [20/90], Loss: 0.9635\n",
      "Epoch [25/50], lter [40/90], Loss: 1.0817\n",
      "Epoch [25/50], lter [60/90], Loss: 0.8387\n",
      "Epoch [25/50], lter [80/90], Loss: 0.7187\n",
      "Epoch [26/50], lter [20/90], Loss: 0.8251\n",
      "Epoch [26/50], lter [40/90], Loss: 0.9770\n",
      "Epoch [26/50], lter [60/90], Loss: 0.9688\n",
      "Epoch [26/50], lter [80/90], Loss: 1.1483\n",
      "Epoch [27/50], lter [20/90], Loss: 0.8540\n",
      "Epoch [27/50], lter [40/90], Loss: 0.8291\n",
      "Epoch [27/50], lter [60/90], Loss: 0.8736\n",
      "Epoch [27/50], lter [80/90], Loss: 1.1449\n",
      "Epoch [28/50], lter [20/90], Loss: 1.0817\n",
      "Epoch [28/50], lter [40/90], Loss: 0.6994\n",
      "Epoch [28/50], lter [60/90], Loss: 1.0633\n",
      "Epoch [28/50], lter [80/90], Loss: 0.9898\n",
      "Epoch [29/50], lter [20/90], Loss: 1.2284\n",
      "Epoch [29/50], lter [40/90], Loss: 0.8736\n",
      "Epoch [29/50], lter [60/90], Loss: 1.0092\n",
      "Epoch [29/50], lter [80/90], Loss: 0.8994\n",
      "Epoch [30/50], lter [20/90], Loss: 0.8564\n",
      "Epoch [30/50], lter [40/90], Loss: 1.3698\n",
      "Epoch [30/50], lter [60/90], Loss: 0.9711\n",
      "Epoch [30/50], lter [80/90], Loss: 1.1496\n",
      "Epoch [31/50], lter [20/90], Loss: 0.8977\n",
      "Epoch [31/50], lter [40/90], Loss: 0.9873\n",
      "Epoch [31/50], lter [60/90], Loss: 0.7635\n",
      "Epoch [31/50], lter [80/90], Loss: 1.2113\n",
      "Epoch [32/50], lter [20/90], Loss: 0.7978\n",
      "Epoch [32/50], lter [40/90], Loss: 1.0666\n",
      "Epoch [32/50], lter [60/90], Loss: 0.8353\n",
      "Epoch [32/50], lter [80/90], Loss: 1.0832\n",
      "Epoch [33/50], lter [20/90], Loss: 0.8981\n",
      "Epoch [33/50], lter [40/90], Loss: 0.7705\n",
      "Epoch [33/50], lter [60/90], Loss: 0.9025\n",
      "Epoch [33/50], lter [80/90], Loss: 1.0564\n",
      "Epoch [34/50], lter [20/90], Loss: 0.9062\n",
      "Epoch [34/50], lter [40/90], Loss: 1.0544\n",
      "Epoch [34/50], lter [60/90], Loss: 0.6701\n",
      "Epoch [34/50], lter [80/90], Loss: 1.0496\n",
      "Epoch [35/50], lter [20/90], Loss: 0.6910\n",
      "Epoch [35/50], lter [40/90], Loss: 0.8026\n",
      "Epoch [35/50], lter [60/90], Loss: 0.9436\n",
      "Epoch [35/50], lter [80/90], Loss: 1.1633\n",
      "Epoch [36/50], lter [20/90], Loss: 0.9182\n",
      "Epoch [36/50], lter [40/90], Loss: 0.7526\n",
      "Epoch [36/50], lter [60/90], Loss: 0.8256\n",
      "Epoch [36/50], lter [80/90], Loss: 0.7301\n",
      "Epoch [37/50], lter [20/90], Loss: 0.9595\n",
      "Epoch [37/50], lter [40/90], Loss: 1.0361\n",
      "Epoch [37/50], lter [60/90], Loss: 0.7376\n",
      "Epoch [37/50], lter [80/90], Loss: 1.0084\n",
      "Epoch [38/50], lter [20/90], Loss: 0.8205\n",
      "Epoch [38/50], lter [40/90], Loss: 0.8464\n",
      "Epoch [38/50], lter [60/90], Loss: 0.9689\n",
      "Epoch [38/50], lter [80/90], Loss: 0.9669\n",
      "Epoch [39/50], lter [20/90], Loss: 0.7237\n",
      "Epoch [39/50], lter [40/90], Loss: 0.9066\n",
      "Epoch [39/50], lter [60/90], Loss: 0.8405\n",
      "Epoch [39/50], lter [80/90], Loss: 0.7581\n",
      "Epoch [40/50], lter [20/90], Loss: 1.0080\n",
      "Epoch [40/50], lter [40/90], Loss: 0.9632\n",
      "Epoch [40/50], lter [60/90], Loss: 0.7930\n",
      "Epoch [40/50], lter [80/90], Loss: 1.0661\n",
      "Epoch [41/50], lter [20/90], Loss: 1.1515\n",
      "Epoch [41/50], lter [40/90], Loss: 1.1118\n",
      "Epoch [41/50], lter [60/90], Loss: 1.0694\n",
      "Epoch [41/50], lter [80/90], Loss: 0.9013\n",
      "Epoch [42/50], lter [20/90], Loss: 0.7878\n",
      "Epoch [42/50], lter [40/90], Loss: 0.8403\n",
      "Epoch [42/50], lter [60/90], Loss: 0.8206\n",
      "Epoch [42/50], lter [80/90], Loss: 1.2584\n",
      "Epoch [43/50], lter [20/90], Loss: 0.9043\n",
      "Epoch [43/50], lter [40/90], Loss: 1.1272\n",
      "Epoch [43/50], lter [60/90], Loss: 0.8319\n",
      "Epoch [43/50], lter [80/90], Loss: 0.7069\n",
      "Epoch [44/50], lter [20/90], Loss: 0.8787\n",
      "Epoch [44/50], lter [40/90], Loss: 0.5892\n",
      "Epoch [44/50], lter [60/90], Loss: 1.1330\n",
      "Epoch [44/50], lter [80/90], Loss: 0.8334\n",
      "Epoch [45/50], lter [20/90], Loss: 0.8532\n",
      "Epoch [45/50], lter [40/90], Loss: 1.2199\n",
      "Epoch [45/50], lter [60/90], Loss: 0.6762\n",
      "Epoch [45/50], lter [80/90], Loss: 0.6785\n",
      "Epoch [46/50], lter [20/90], Loss: 0.6582\n",
      "Epoch [46/50], lter [40/90], Loss: 0.7298\n",
      "Epoch [46/50], lter [60/90], Loss: 0.9589\n",
      "Epoch [46/50], lter [80/90], Loss: 1.1725\n",
      "Epoch [47/50], lter [20/90], Loss: 0.6716\n",
      "Epoch [47/50], lter [40/90], Loss: 0.9645\n",
      "Epoch [47/50], lter [60/90], Loss: 0.9001\n",
      "Epoch [47/50], lter [80/90], Loss: 0.7621\n",
      "Epoch [48/50], lter [20/90], Loss: 1.3157\n",
      "Epoch [48/50], lter [40/90], Loss: 0.8185\n",
      "Epoch [48/50], lter [60/90], Loss: 0.9309\n",
      "Epoch [48/50], lter [80/90], Loss: 0.7787\n",
      "Epoch [49/50], lter [20/90], Loss: 0.8091\n",
      "Epoch [49/50], lter [40/90], Loss: 0.6218\n",
      "Epoch [49/50], lter [60/90], Loss: 0.6750\n",
      "Epoch [49/50], lter [80/90], Loss: 0.9438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], lter [20/90], Loss: 0.9618\n",
      "Epoch [50/50], lter [40/90], Loss: 0.9584\n",
      "Epoch [50/50], lter [60/90], Loss: 0.8582\n",
      "Epoch [50/50], lter [80/90], Loss: 0.5748\n",
      "Learning Finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, (batch_text, batch_labels) in enumerate(train_loader):\n",
    "        X = batch_text # if cuda -> X = batch_text.cuda()\n",
    "        Y = batch_labels # if cuda -> Y = batch_text.cuda()\n",
    "        \n",
    "        pre = model(X)\n",
    "        cost = loss(pre, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "print(\"Learning Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "x_test = x_test.view(-1, 1, 12, 500)\n",
    "\n",
    "y_test = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "\n",
    "test_data = Data.TensorDataset(x_test, y_test)\n",
    "\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test text: 55.445545 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for text, labels in test_loader:\n",
    "    # if cuda -> text = text.cuda()\n",
    "    \n",
    "    outputs = model(text)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += 1\n",
    "    correct += (predicted == labels).sum() # if cuda -> correct += (pre == labels.cuda()).sum()\n",
    "    \n",
    "print('Accuracy of test text: %f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
