{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Okt as Twitter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# G마켓에 좋은 평만 있어서 사봤는데.. 정말 진짜 진짜 사지마세요. 개. 쓰. 레. 기 (진심) 입니다. 액정부터 짜증나는 TN패널에, 하드 SSD인걸로 알았는데, 속도는 저질 SD카드 꽂아 놓은것 같습니다. 정말 느려터집니다. 저는 단지 인터넷 뱅킹만 할려고, 샀단 말입니다. 그런데 인터넷 뱅킹 프로그램까는데만 10~20분 걸립니다. 뭐약!! 이게!! 분노로 인해 볼때마다 짜증납니다. 밤에 잠도 안오고요.. 사시면 분명 후회하실겁니다. 아! 진짜 G마켓 프리미엄평으로 실날하게 사진찍어서 올리려고했는데, 먹고 산다고 바빠서 프리미엄 평 못 올린게 정말 천추의 한이네요!!\\n# 원래 그런 줄 알고 사는 \"저가 제품\"이라고 생각합니다만. IPS라는 언급이 없으니 당연히 TN 패널일 테고, EMMC는 SSD가 아니고 SD 카드 내장된 것 같은 것이라 원래 SSD보다 느린 것이고, CPU도 아톰이니 뭐 당연히 느리죠. 그런 것 다 감안하고 \"싸고 가볍다\"는 조건으로 사는 제품인데요. 뭐 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = codecs.open('data/reviews.txt', 'r', 'utf-8')\n",
    "f.read()[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"발열\", \"소음\"]\n",
    "\n",
    "for keyword in keywords:\n",
    "    temp_list = []\n",
    "    save_name = \"data/reviews_\" + keyword + \".txt\"\n",
    "    f = codecs.open(\"data/reviews.txt\", \"r\", \"utf-8\")\n",
    "    t = codecs.open(save_name, \"w\", \"utf-8\")\n",
    "    \n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        if keyword in line:\n",
    "            temp_list.append(line)\n",
    "    set_list = list(set(temp_list))\n",
    "    \n",
    "    for item in set_list:\n",
    "        t.write(item)\n",
    "\n",
    "    f.close()\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 노트북 진짜로 디자인 깔끔하고 이뻐요! 오피스깔고 기타프로그램 설치하는것도 빨라요! 써봐야 알겠지만 발열도 거의 없고 쿨링팬 소음도 거의 없어요! 노트북 어느정도 크긴 큰데 상당히 가벼워요 대박! 가성비 짱짱 인거 같아연>\\n# 맨처음 상자 열어서 꺼내 들었을 때 깜짝놀랐어요 너무 가벼워요.얘가 넘 연약한거 아닌가 장난감인가 싶을 정도로 정말 가벼워서 놀랬어요 왠만한 책보다 가벼워요.렘 업그레이드 추가해서 구매했는데 잘 되어서 왔구요 .엄청 빠르고 좋아요. 소음도 한개도 없어요. 정말 조용해요 발열은 캠 있는부분 조금 따뜻한 느낌으로 있구요. 크게 신경쓰일 정도는 아니에요 노트북 가볍고 좋습니다.\\n# 맥북, 그램, 노트북9 중 뭐살지 일주일을 고민하다 결국 노트북9으로 결정했습니다.램도 듀얼채널이고 발열이나 터치패드 감도나 키감 등이 다 최상위권 인것 같습니다. 무게에 너무 집중하여 많은 기본기를 잃은 그램보다는 노트북9이 한수 위라고 생각되네요.맥북하고 고민은 많이 했습니다. 디'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = codecs.open('data/reviews_발열.txt', 'r', 'utf-8')\n",
    "f.read()[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Scored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/score_발열.xlsx'\n",
    "sheet_name = 'Sheet1'\n",
    "data = pd.read_excel(filename, sheet_name=sheet_name, header=0)\n",
    "\n",
    "csv_data = [item.replace('#', '').strip() for item in data['Review']]\n",
    "csv_label = data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['발열히 심한거 같은데 여름이라 그런가?..',\n",
       "  '발열이좀 심한거 같아서 걱정이에요',\n",
       "  '발열이심하더라구요',\n",
       "  '발열이너무심한게 제일큰 단점인것 같고 그외에 불편한점은',\n",
       "  '발열이...정말...심합니다'],\n",
       " 0    2\n",
       " 1    2\n",
       " 2    2\n",
       " 3    2\n",
       " 4    2\n",
       " Name: Score, dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data[:5], csv_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[703, 370, 138]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [sum(csv_label == 0), sum(csv_label == 1), sum(csv_label == 2)]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(scores)), scores, color = \"#ff5733\")\n",
    "plt.title(\"Average Laptop's Score Of Heat\")\n",
    "plt.xlabel(\"Heat Score\")\n",
    "plt.ylabel(\"Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "doc = []\n",
    "\n",
    "for sentence in csv_data:\n",
    "    results = []\n",
    "    tokens = twitter.pos(sentence, norm=True, stem=True)\n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "    doc.append(\" \".join(results).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['발열 히 심하다 같다 여름 그 런가',\n",
       " '발열 이 좀 심하다 같다 걱정',\n",
       " '발열 심하다',\n",
       " '발열 이 너 무심하다 제일 크다 단점 것 같다 그 외 불편하다 점',\n",
       " '발열 정말 심하다']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "cnv = CountVectorizer(ngram_range=(1, 1), min_df = 3)\n",
    "\n",
    "data = cnv.fit_transform(doc).toarray()\n",
    "print(data[0])\n",
    "label = csv_label.values\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'발열': 112,\n",
       " '심하다': 156,\n",
       " '같다': 13,\n",
       " '여름': 187,\n",
       " '걱정': 16,\n",
       " '무심하다': 104,\n",
       " '크다': 266,\n",
       " '단점': 57,\n",
       " '불편하다': 125,\n",
       " '정말': 237,\n",
       " '확실하다': 292,\n",
       " '심해': 157,\n",
       " '노트북': 45,\n",
       " '쿨러': 263,\n",
       " '사용': 134,\n",
       " '크게': 265,\n",
       " '진짜': 250,\n",
       " '너무': 43,\n",
       " '많이': 93,\n",
       " '있다': 218,\n",
       " '장시간': 228,\n",
       " '높다': 47,\n",
       " '프로그램': 275,\n",
       " '한편': 283,\n",
       " '이다': 204,\n",
       " '심다': 155,\n",
       " '밧데리': 113,\n",
       " 'cpu': 4,\n",
       " '용량': 200,\n",
       " '빼다': 131,\n",
       " '괜찮다': 22,\n",
       " '느낌': 50,\n",
       " '하지만': 281,\n",
       " '자판': 221,\n",
       " '조금': 241,\n",
       " '속도': 149,\n",
       " '매우': 94,\n",
       " '좋다': 244,\n",
       " '이전': 211,\n",
       " '하다': 279,\n",
       " '뜨겁다': 83,\n",
       " '가볍다': 10,\n",
       " '조용하다': 242,\n",
       " '마음': 88,\n",
       " '듭니': 72,\n",
       " '돌아가다': 65,\n",
       " '불량': 124,\n",
       " '심해지다': 158,\n",
       " '생기다': 141,\n",
       " '소리': 147,\n",
       " '나다': 38,\n",
       " '그렇다': 31,\n",
       " '부분': 122,\n",
       " '문제': 107,\n",
       " '끊기다': 37,\n",
       " '쓰다': 160,\n",
       " '약간': 175,\n",
       " '게임': 18,\n",
       " '돌리다': 64,\n",
       " '정도': 236,\n",
       " '성능': 145,\n",
       " '라면': 85,\n",
       " '지다': 249,\n",
       " '이렇다': 206,\n",
       " '부팅': 123,\n",
       " '화상': 289,\n",
       " '원래': 202,\n",
       " '모르다': 100,\n",
       " '않다': 171,\n",
       " '구매': 24,\n",
       " '신분': 154,\n",
       " '야하다': 174,\n",
       " '제외': 239,\n",
       " '자주': 220,\n",
       " '발생': 111,\n",
       " '삼성': 137,\n",
       " '소음': 148,\n",
       " '치다': 259,\n",
       " '만족하다': 91,\n",
       " '배터리': 116,\n",
       " '빠르다': 128,\n",
       " '무엇': 105,\n",
       " '생각': 140,\n",
       " '하드': 280,\n",
       " '보드': 119,\n",
       " '왼쪽': 199,\n",
       " '상당하다': 138,\n",
       " '살짝': 136,\n",
       " '메탈': 95,\n",
       " '비다': 126,\n",
       " '비추다': 127,\n",
       " '휴대': 294,\n",
       " '다니다': 54,\n",
       " '많다': 92,\n",
       " '느리다': 51,\n",
       " '다른': 55,\n",
       " '제품': 240,\n",
       " '시간': 152,\n",
       " '센터': 146,\n",
       " '늘다': 52,\n",
       " '이제': 213,\n",
       " '조절': 243,\n",
       " '약하다': 176,\n",
       " '어쩔': 182,\n",
       " '전혀': 233,\n",
       " '잡지': 226,\n",
       " '해보다': 284,\n",
       " '편이': 271,\n",
       " '되다': 67,\n",
       " '화이트': 290,\n",
       " '다소': 56,\n",
       " '느껴지다': 48,\n",
       " '하나': 277,\n",
       " '따르다': 77,\n",
       " '내다': 42,\n",
       " '사양': 133,\n",
       " '낮다': 41,\n",
       " '양호': 178,\n",
       " '상태': 139,\n",
       " '차이': 253,\n",
       " '보다': 118,\n",
       " '수가': 150,\n",
       " '없다': 185,\n",
       " '인하다': 215,\n",
       " '지금': 248,\n",
       " '어떻다': 181,\n",
       " '또한': 81,\n",
       " '가격': 8,\n",
       " '때문': 79,\n",
       " '켜다': 262,\n",
       " '충전': 258,\n",
       " '능력': 53,\n",
       " '떨어지다': 80,\n",
       " '오래되다': 194,\n",
       " '자다': 219,\n",
       " '관리': 21,\n",
       " '돼다': 66,\n",
       " '걸리다': 17,\n",
       " '아니다': 163,\n",
       " '안되다': 169,\n",
       " '그냥': 26,\n",
       " '받침': 110,\n",
       " '절대': 234,\n",
       " '보이다': 120,\n",
       " '고민': 20,\n",
       " '빠지다': 130,\n",
       " '모두': 98,\n",
       " '한성': 282,\n",
       " '구성': 25,\n",
       " 'as': 3,\n",
       " '굉장하다': 23,\n",
       " '깔끔하다': 36,\n",
       " '이쁘다': 208,\n",
       " '모델': 97,\n",
       " '현상': 286,\n",
       " '50': 1,\n",
       " '준수': 246,\n",
       " '처리': 254,\n",
       " '잡다': 225,\n",
       " '얇다': 177,\n",
       " '오래': 193,\n",
       " '쓸다': 162,\n",
       " '일이': 217,\n",
       " '인터넷': 214,\n",
       " '100': 0,\n",
       " '그래픽': 28,\n",
       " '장점': 229,\n",
       " '훨씬': 293,\n",
       " '해소': 285,\n",
       " '패드': 269,\n",
       " '일어나서': 216,\n",
       " '쿨링': 264,\n",
       " '이상': 209,\n",
       " '기본': 35,\n",
       " '서비스': 142,\n",
       " '들다': 71,\n",
       " '사은': 135,\n",
       " '주다': 245,\n",
       " '어느': 180,\n",
       " '문서': 106,\n",
       " '작업': 223,\n",
       " '편입': 272,\n",
       " '아주': 166,\n",
       " '컴퓨터': 261,\n",
       " '배송': 115,\n",
       " '이정': 212,\n",
       " '무게': 102,\n",
       " '그램': 29,\n",
       " '두다': 69,\n",
       " '온도': 195,\n",
       " '아쉽다': 164,\n",
       " '그래도': 27,\n",
       " '예쁘다': 191,\n",
       " '따다': 75,\n",
       " '안나': 168,\n",
       " '디자인': 74,\n",
       " '가다': 9,\n",
       " '편하다': 273,\n",
       " '겨울': 19,\n",
       " '이면': 207,\n",
       " '나쁘다': 39,\n",
       " '필요': 276,\n",
       " '받다': 109,\n",
       " '액정': 173,\n",
       " '돌다': 63,\n",
       " '아예': 165,\n",
       " '레노버': 86,\n",
       " '처음': 255,\n",
       " '써다': 159,\n",
       " '얘기': 179,\n",
       " '느끼다': 49,\n",
       " '나오다': 40,\n",
       " '등등': 73,\n",
       " '데스크탑': 62,\n",
       " '연결': 189,\n",
       " '아직': 167,\n",
       " '수준': 151,\n",
       " '뜨다': 84,\n",
       " '사다': 132,\n",
       " '설치': 144,\n",
       " '평소': 274,\n",
       " '70': 2,\n",
       " '신경': 153,\n",
       " '쓰이다': 161,\n",
       " '노트': 44,\n",
       " '따뜻하다': 76,\n",
       " '최고': 256,\n",
       " 'ssd': 6,\n",
       " '카드': 260,\n",
       " '마감': 87,\n",
       " '없이': 186,\n",
       " '윈도우': 203,\n",
       " '보통': 121,\n",
       " '펴다': 270,\n",
       " '감다': 12,\n",
       " '별로': 117,\n",
       " '화면': 288,\n",
       " 'ㅜㅜ': 7,\n",
       " '엄청': 183,\n",
       " '개선': 14,\n",
       " '차다': 252,\n",
       " '완벽하다': 196,\n",
       " '열량': 190,\n",
       " '빠릿빠릿': 129,\n",
       " '그렇게': 30,\n",
       " '기능': 33,\n",
       " '오다': 192,\n",
       " '모니터': 96,\n",
       " '적당하다': 232,\n",
       " '던지다': 61,\n",
       " '거의': 15,\n",
       " '무겁다': 101,\n",
       " '추천': 257,\n",
       " '만족': 89,\n",
       " '그리다': 32,\n",
       " '대다': 58,\n",
       " '대비': 60,\n",
       " '딱하다': 78,\n",
       " '뒤틀리다': 70,\n",
       " '짧다': 251,\n",
       " '모든': 99,\n",
       " '제어': 238,\n",
       " '완전': 197,\n",
       " '기다': 34,\n",
       " '알다': 172,\n",
       " '가성': 11,\n",
       " '잡히다': 227,\n",
       " '우수하다': 201,\n",
       " '증상': 247,\n",
       " '뛰어나다': 82,\n",
       " '되어다': 68,\n",
       " '적다': 231,\n",
       " '선택': 143,\n",
       " '만족스럽다': 90,\n",
       " '현재': 287,\n",
       " '화질': 291,\n",
       " '특히': 267,\n",
       " '하니': 278,\n",
       " '이렇게': 205,\n",
       " '놀라다': 46,\n",
       " '점수': 235,\n",
       " '물론': 108,\n",
       " '무리': 103,\n",
       " '외관': 198,\n",
       " '대단하다': 59,\n",
       " '역시': 188,\n",
       " '작다': 222,\n",
       " '저렴하다': 230,\n",
       " '잡고': 224,\n",
       " '엄청나다': 184,\n",
       " 'ips': 5,\n",
       " '패널': 268,\n",
       " '안심': 170,\n",
       " '배그': 114,\n",
       " '이슈': 210}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(cnv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label, stratify = label)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(train_data).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(train_label).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([908]), torch.Size([908, 295]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size(), x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data.TensorDataset(x, y)\n",
    "batch_size = 10\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = nn.Linear(dim, 200, bias=True)\n",
    "linear2 = nn.Linear(200, 3, bias=True)\n",
    "\n",
    "relu = nn.ReLU()\n",
    "\n",
    "model = nn.Sequential(linear1, relu, linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], lter [20/90] Loss: 0.9397\n",
      "Epoch [1/50], lter [40/90] Loss: 1.0582\n",
      "Epoch [1/50], lter [60/90] Loss: 0.6918\n",
      "Epoch [1/50], lter [80/90] Loss: 0.9955\n",
      "Epoch [2/50], lter [20/90] Loss: 0.5593\n",
      "Epoch [2/50], lter [40/90] Loss: 0.7867\n",
      "Epoch [2/50], lter [60/90] Loss: 0.7392\n",
      "Epoch [2/50], lter [80/90] Loss: 0.5639\n",
      "Epoch [3/50], lter [20/90] Loss: 0.5588\n",
      "Epoch [3/50], lter [40/90] Loss: 0.2007\n",
      "Epoch [3/50], lter [60/90] Loss: 0.5443\n",
      "Epoch [3/50], lter [80/90] Loss: 0.7605\n",
      "Epoch [4/50], lter [20/90] Loss: 0.3347\n",
      "Epoch [4/50], lter [40/90] Loss: 0.2876\n",
      "Epoch [4/50], lter [60/90] Loss: 0.3186\n",
      "Epoch [4/50], lter [80/90] Loss: 0.1204\n",
      "Epoch [5/50], lter [20/90] Loss: 0.2242\n",
      "Epoch [5/50], lter [40/90] Loss: 0.2342\n",
      "Epoch [5/50], lter [60/90] Loss: 0.2593\n",
      "Epoch [5/50], lter [80/90] Loss: 0.2537\n",
      "Epoch [6/50], lter [20/90] Loss: 0.1300\n",
      "Epoch [6/50], lter [40/90] Loss: 0.0357\n",
      "Epoch [6/50], lter [60/90] Loss: 0.0259\n",
      "Epoch [6/50], lter [80/90] Loss: 0.1675\n",
      "Epoch [7/50], lter [20/90] Loss: 0.2138\n",
      "Epoch [7/50], lter [40/90] Loss: 0.4376\n",
      "Epoch [7/50], lter [60/90] Loss: 0.0965\n",
      "Epoch [7/50], lter [80/90] Loss: 0.0560\n",
      "Epoch [8/50], lter [20/90] Loss: 0.1270\n",
      "Epoch [8/50], lter [40/90] Loss: 0.3336\n",
      "Epoch [8/50], lter [60/90] Loss: 0.0553\n",
      "Epoch [8/50], lter [80/90] Loss: 0.1827\n",
      "Epoch [9/50], lter [20/90] Loss: 0.0418\n",
      "Epoch [9/50], lter [40/90] Loss: 0.0410\n",
      "Epoch [9/50], lter [60/90] Loss: 0.0784\n",
      "Epoch [9/50], lter [80/90] Loss: 0.0537\n",
      "Epoch [10/50], lter [20/90] Loss: 0.0255\n",
      "Epoch [10/50], lter [40/90] Loss: 0.0887\n",
      "Epoch [10/50], lter [60/90] Loss: 0.0166\n",
      "Epoch [10/50], lter [80/90] Loss: 0.1605\n",
      "Epoch [11/50], lter [20/90] Loss: 0.2510\n",
      "Epoch [11/50], lter [40/90] Loss: 0.1777\n",
      "Epoch [11/50], lter [60/90] Loss: 0.0817\n",
      "Epoch [11/50], lter [80/90] Loss: 0.1495\n",
      "Epoch [12/50], lter [20/90] Loss: 0.0172\n",
      "Epoch [12/50], lter [40/90] Loss: 0.2455\n",
      "Epoch [12/50], lter [60/90] Loss: 0.1127\n",
      "Epoch [12/50], lter [80/90] Loss: 0.0146\n",
      "Epoch [13/50], lter [20/90] Loss: 0.0710\n",
      "Epoch [13/50], lter [40/90] Loss: 0.2949\n",
      "Epoch [13/50], lter [60/90] Loss: 0.0410\n",
      "Epoch [13/50], lter [80/90] Loss: 0.2250\n",
      "Epoch [14/50], lter [20/90] Loss: 0.0416\n",
      "Epoch [14/50], lter [40/90] Loss: 0.0392\n",
      "Epoch [14/50], lter [60/90] Loss: 0.0666\n",
      "Epoch [14/50], lter [80/90] Loss: 0.1212\n",
      "Epoch [15/50], lter [20/90] Loss: 0.0504\n",
      "Epoch [15/50], lter [40/90] Loss: 0.0308\n",
      "Epoch [15/50], lter [60/90] Loss: 0.1780\n",
      "Epoch [15/50], lter [80/90] Loss: 0.1136\n",
      "Epoch [16/50], lter [20/90] Loss: 0.1210\n",
      "Epoch [16/50], lter [40/90] Loss: 0.0219\n",
      "Epoch [16/50], lter [60/90] Loss: 0.2426\n",
      "Epoch [16/50], lter [80/90] Loss: 0.0387\n",
      "Epoch [17/50], lter [20/90] Loss: 0.3101\n",
      "Epoch [17/50], lter [40/90] Loss: 0.0223\n",
      "Epoch [17/50], lter [60/90] Loss: 0.0493\n",
      "Epoch [17/50], lter [80/90] Loss: 0.0724\n",
      "Epoch [18/50], lter [20/90] Loss: 0.0093\n",
      "Epoch [18/50], lter [40/90] Loss: 0.0206\n",
      "Epoch [18/50], lter [60/90] Loss: 0.2089\n",
      "Epoch [18/50], lter [80/90] Loss: 0.0450\n",
      "Epoch [19/50], lter [20/90] Loss: 0.0260\n",
      "Epoch [19/50], lter [40/90] Loss: 0.0639\n",
      "Epoch [19/50], lter [60/90] Loss: 0.0568\n",
      "Epoch [19/50], lter [80/90] Loss: 0.0962\n",
      "Epoch [20/50], lter [20/90] Loss: 0.0865\n",
      "Epoch [20/50], lter [40/90] Loss: 0.2036\n",
      "Epoch [20/50], lter [60/90] Loss: 0.1184\n",
      "Epoch [20/50], lter [80/90] Loss: 0.0877\n",
      "Epoch [21/50], lter [20/90] Loss: 0.1035\n",
      "Epoch [21/50], lter [40/90] Loss: 0.0143\n",
      "Epoch [21/50], lter [60/90] Loss: 0.0011\n",
      "Epoch [21/50], lter [80/90] Loss: 0.0076\n",
      "Epoch [22/50], lter [20/90] Loss: 0.0146\n",
      "Epoch [22/50], lter [40/90] Loss: 0.0459\n",
      "Epoch [22/50], lter [60/90] Loss: 0.0193\n",
      "Epoch [22/50], lter [80/90] Loss: 0.0037\n",
      "Epoch [23/50], lter [20/90] Loss: 0.2572\n",
      "Epoch [23/50], lter [40/90] Loss: 0.1219\n",
      "Epoch [23/50], lter [60/90] Loss: 0.0070\n",
      "Epoch [23/50], lter [80/90] Loss: 0.0407\n",
      "Epoch [24/50], lter [20/90] Loss: 0.0198\n",
      "Epoch [24/50], lter [40/90] Loss: 0.0292\n",
      "Epoch [24/50], lter [60/90] Loss: 0.1019\n",
      "Epoch [24/50], lter [80/90] Loss: 0.0251\n",
      "Epoch [25/50], lter [20/90] Loss: 0.2734\n",
      "Epoch [25/50], lter [40/90] Loss: 0.0072\n",
      "Epoch [25/50], lter [60/90] Loss: 0.0455\n",
      "Epoch [25/50], lter [80/90] Loss: 0.0672\n",
      "Epoch [26/50], lter [20/90] Loss: 0.0506\n",
      "Epoch [26/50], lter [40/90] Loss: 0.0301\n",
      "Epoch [26/50], lter [60/90] Loss: 0.0777\n",
      "Epoch [26/50], lter [80/90] Loss: 0.1269\n",
      "Epoch [27/50], lter [20/90] Loss: 0.1848\n",
      "Epoch [27/50], lter [40/90] Loss: 0.3114\n",
      "Epoch [27/50], lter [60/90] Loss: 0.1486\n",
      "Epoch [27/50], lter [80/90] Loss: 0.0974\n",
      "Epoch [28/50], lter [20/90] Loss: 0.0604\n",
      "Epoch [28/50], lter [40/90] Loss: 0.0875\n",
      "Epoch [28/50], lter [60/90] Loss: 0.0635\n",
      "Epoch [28/50], lter [80/90] Loss: 0.0265\n",
      "Epoch [29/50], lter [20/90] Loss: 0.0006\n",
      "Epoch [29/50], lter [40/90] Loss: 0.0438\n",
      "Epoch [29/50], lter [60/90] Loss: 0.0072\n",
      "Epoch [29/50], lter [80/90] Loss: 0.0688\n",
      "Epoch [30/50], lter [20/90] Loss: 0.1097\n",
      "Epoch [30/50], lter [40/90] Loss: 0.0215\n",
      "Epoch [30/50], lter [60/90] Loss: 0.0049\n",
      "Epoch [30/50], lter [80/90] Loss: 0.0014\n",
      "Epoch [31/50], lter [20/90] Loss: 0.3104\n",
      "Epoch [31/50], lter [40/90] Loss: 0.0026\n",
      "Epoch [31/50], lter [60/90] Loss: 0.0924\n",
      "Epoch [31/50], lter [80/90] Loss: 0.1702\n",
      "Epoch [32/50], lter [20/90] Loss: 0.0018\n",
      "Epoch [32/50], lter [40/90] Loss: 0.0010\n",
      "Epoch [32/50], lter [60/90] Loss: 0.1434\n",
      "Epoch [32/50], lter [80/90] Loss: 0.0176\n",
      "Epoch [33/50], lter [20/90] Loss: 0.0915\n",
      "Epoch [33/50], lter [40/90] Loss: 0.0016\n",
      "Epoch [33/50], lter [60/90] Loss: 0.0006\n",
      "Epoch [33/50], lter [80/90] Loss: 0.0119\n",
      "Epoch [34/50], lter [20/90] Loss: 0.0256\n",
      "Epoch [34/50], lter [40/90] Loss: 0.0039\n",
      "Epoch [34/50], lter [60/90] Loss: 0.1311\n",
      "Epoch [34/50], lter [80/90] Loss: 0.0127\n",
      "Epoch [35/50], lter [20/90] Loss: 0.0339\n",
      "Epoch [35/50], lter [40/90] Loss: 0.0031\n",
      "Epoch [35/50], lter [60/90] Loss: 0.2653\n",
      "Epoch [35/50], lter [80/90] Loss: 0.0746\n",
      "Epoch [36/50], lter [20/90] Loss: 0.0207\n",
      "Epoch [36/50], lter [40/90] Loss: 0.0165\n",
      "Epoch [36/50], lter [60/90] Loss: 0.1627\n",
      "Epoch [36/50], lter [80/90] Loss: 0.0015\n",
      "Epoch [37/50], lter [20/90] Loss: 0.2720\n",
      "Epoch [37/50], lter [40/90] Loss: 0.0059\n",
      "Epoch [37/50], lter [60/90] Loss: 0.0129\n",
      "Epoch [37/50], lter [80/90] Loss: 0.0258\n",
      "Epoch [38/50], lter [20/90] Loss: 0.0605\n",
      "Epoch [38/50], lter [40/90] Loss: 0.0136\n",
      "Epoch [38/50], lter [60/90] Loss: 0.1297\n",
      "Epoch [38/50], lter [80/90] Loss: 0.0011\n",
      "Epoch [39/50], lter [20/90] Loss: 0.0175\n",
      "Epoch [39/50], lter [40/90] Loss: 0.1273\n",
      "Epoch [39/50], lter [60/90] Loss: 0.0002\n",
      "Epoch [39/50], lter [80/90] Loss: 0.0487\n",
      "Epoch [40/50], lter [20/90] Loss: 0.0018\n",
      "Epoch [40/50], lter [40/90] Loss: 0.0700\n",
      "Epoch [40/50], lter [60/90] Loss: 0.2771\n",
      "Epoch [40/50], lter [80/90] Loss: 0.4847\n",
      "Epoch [41/50], lter [20/90] Loss: 0.0686\n",
      "Epoch [41/50], lter [40/90] Loss: 0.0546\n",
      "Epoch [41/50], lter [60/90] Loss: 0.0925\n",
      "Epoch [41/50], lter [80/90] Loss: 0.1455\n",
      "Epoch [42/50], lter [20/90] Loss: 0.1863\n",
      "Epoch [42/50], lter [40/90] Loss: 0.0729\n",
      "Epoch [42/50], lter [60/90] Loss: 0.0465\n",
      "Epoch [42/50], lter [80/90] Loss: 0.0103\n",
      "Epoch [43/50], lter [20/90] Loss: 0.2243\n",
      "Epoch [43/50], lter [40/90] Loss: 0.0072\n",
      "Epoch [43/50], lter [60/90] Loss: 0.0026\n",
      "Epoch [43/50], lter [80/90] Loss: 0.0021\n",
      "Epoch [44/50], lter [20/90] Loss: 0.0075\n",
      "Epoch [44/50], lter [40/90] Loss: 0.3901\n",
      "Epoch [44/50], lter [60/90] Loss: 0.0038\n",
      "Epoch [44/50], lter [80/90] Loss: 0.0097\n",
      "Epoch [45/50], lter [20/90] Loss: 0.0046\n",
      "Epoch [45/50], lter [40/90] Loss: 0.0105\n",
      "Epoch [45/50], lter [60/90] Loss: 0.0645\n",
      "Epoch [45/50], lter [80/90] Loss: 0.0011\n",
      "Epoch [46/50], lter [20/90] Loss: 0.0956\n",
      "Epoch [46/50], lter [40/90] Loss: 0.0007\n",
      "Epoch [46/50], lter [60/90] Loss: 0.0014\n",
      "Epoch [46/50], lter [80/90] Loss: 0.3716\n",
      "Epoch [47/50], lter [20/90] Loss: 0.0176\n",
      "Epoch [47/50], lter [40/90] Loss: 0.0010\n",
      "Epoch [47/50], lter [60/90] Loss: 0.0046\n",
      "Epoch [47/50], lter [80/90] Loss: 0.0131\n",
      "Epoch [48/50], lter [20/90] Loss: 0.0367\n",
      "Epoch [48/50], lter [40/90] Loss: 0.0047\n",
      "Epoch [48/50], lter [60/90] Loss: 0.0146\n",
      "Epoch [48/50], lter [80/90] Loss: 0.1327\n",
      "Epoch [49/50], lter [20/90] Loss: 0.0018\n",
      "Epoch [49/50], lter [40/90] Loss: 0.1641\n",
      "Epoch [49/50], lter [60/90] Loss: 0.0031\n",
      "Epoch [49/50], lter [80/90] Loss: 0.0798\n",
      "Epoch [50/50], lter [20/90] Loss: 0.0037\n",
      "Epoch [50/50], lter [40/90] Loss: 0.0241\n",
      "Epoch [50/50], lter [60/90] Loss: 0.0012\n",
      "Epoch [50/50], lter [80/90] Loss: 0.0071\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_batch = len(train_data) // batch_size\n",
    "    for i, (batch_text, batch_labels) in enumerate(train_loader):\n",
    "        X = batch_text.view(-1, dim)\n",
    "        Y = batch_labels\n",
    "        \n",
    "        pre = model(X)\n",
    "        cost = loss(pre, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "    \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "test_data = Data.TensorDataset(x_test, y_test)\n",
    "\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test text: 82.508251 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for text, labels in test_loader:\n",
    "    text = text.view(-1, dim)\n",
    "    outputs = model(text)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += 1\n",
    "    \n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print(\"Accuracy of test text: %f %%\" % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsitHot(string):\n",
    "    results = []\n",
    "    tokens = twitter.pos(string, norm=True, stem=True)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "            \n",
    "    sample = cnv.transform([\" \".join(results).strip()]).toarray()\n",
    "    sample = torch.from_numpy(sample).type(torch.FloatTensor)\n",
    "    outputs = model(sample)\n",
    "    pre = torch.max(outputs.data, 1)[1].numpy()\n",
    "    \n",
    "    if pre == 0:\n",
    "        print(\"분석 결과: 발열 거의 없음\")\n",
    "    elif pre == 1:\n",
    "        print(\"분석 결과: 발열 조금 있음\")\n",
    "    else:\n",
    "        print(\"분석 결과: 발열 매우 심함\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 결과: 발열 매우 심함\n"
     ]
    }
   ],
   "source": [
    "IsitHot(\"노트북이 금방 뜨거워지더라고요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 결과: 발열 거의 없음\n"
     ]
    }
   ],
   "source": [
    "IsitHot(\"발열이 없는거 같이 느껴지네요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
